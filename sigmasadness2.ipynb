{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Defiying "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9 CTB Computing Assessment**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The scenario that I chose for this task is Covid-19 cases and its impact on differnet countries and geographical regions and I am analysing this data to figure out  different countries faced Covid-19, and how the effects it had. I will analyse this in CSV format.\n",
    "\n",
    "##### Link to dataset https://www.kaggle.com/datasets/imdevskp/corona-virus-report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function requirements**\n",
    "\n",
    "### Data Loading\n",
    "Load Data: Implement functions to load data from CSV files.\\\n",
    "Validate Data: Ensure the loaded data is in a structured format suitable for analysis.\n",
    "#### Data Cleaning\n",
    "Filtering: Implement functions to filter data based on specified criteria.\\\n",
    "Grouping: Implement functions to group data by different categories.\\\n",
    "Handling Missing Values: Develop strategies for handling missing or incomplete data.\\\n",
    "Data Transformation: Ensure data is in the correct format for analysis\n",
    "### Data Analysis\n",
    " Statistical Analysis: Implement functions to perform basic statistical analysis (mean, median, mode, standard deviation).\\\n",
    " Trend Analysis: Identify and analyze trends within the data.\\\n",
    " Comparative Analysis: Compare different groups within the data to identify significant differences.\n",
    "### Data visualization:\n",
    " Display the data through graphs / charts for easier visualization.\n",
    "### Data reporting:\n",
    " Generate and save graphs / charts in appropriate formats such as PNG, JPEG, PDF\\\n",
    " Provide a comprehensive statistical summary of the analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Loading**\n",
    "#### Use Cases \n",
    "##### Actor: User\n",
    "##### Goal: To load a dataset into the system.\n",
    "##### Preconditions: User has a dataset file ready.\n",
    "##### Main Flow:\n",
    "##### 1. User selects the \"Load Data\" option\n",
    "##### 2. User chooses a CSV file from local storage.\n",
    "##### 3. System reads and validates the data.\n",
    "###### Postconditions: Dataset is loaded and ready for analysis.\n",
    "\n",
    "\n",
    "#### **Data Analysis**\n",
    "#### Use Cases \n",
    "##### Actor: User\n",
    "##### Goal: To analyze the loaded dataset for specific information.\n",
    "##### Preconditions: Dataset is loaded and ready for analysis.\n",
    "##### Main Flow:\n",
    "##### 1. User selects the \"Analyze Data\" option.\n",
    "##### 2. User chooses the type of analysis (e.g., statistical, trend, comparative).\n",
    "##### 3. System performs the chosen analysis and displays the results.\n",
    "###### Postconditions: Analysis results are available for review.\n",
    "\n",
    "\n",
    "#### **Data Cleaning**\n",
    "#### Use Cases \n",
    "##### Actor: User\n",
    "##### Goal: To filter out unnecessary information from the dataset\n",
    "##### Preconditions: Dataset is loaded and ready for cleaning.\n",
    "##### Main Flow:\n",
    "##### 1. User selects the \"Clean Data\" option.\n",
    "##### 2. User specifies criteria for filtering, grouping, or handling missing values.\n",
    "##### 3. System processes the data according to the specified criteria.\n",
    "###### Postconditions: The dataset is cleaned of all unnecessary information\n",
    "\n",
    "#### **Data Visualisation**\n",
    "#### Use Cases \n",
    "##### Actor: User\n",
    "##### Goal: To visualize the dataset in a simple way\n",
    "##### Preconditions:Analysis results are available.\n",
    "##### Main Flow:\n",
    "##### 1. User selects the \"Visualize Data\" option.\n",
    "##### 2. User chooses the type of visualization (e.g., graph, chart).\n",
    "##### 3. System generates and displays the chosen visualization.\n",
    "###### Postconditions: The data is visualised in a simple way such as in a graph or chart\n",
    "\n",
    "#### **Data Reporting**\n",
    "#### Use Cases \n",
    "##### Actor: User\n",
    "##### Goal: To generate and save reports of the analysis.\n",
    "##### Preconditions: Analysis and visualizations are available.\n",
    "##### Main Flow:\n",
    "##### 1. User selects the \"Generate Report\" option.\n",
    "##### 2. User specifies the format (e.g., PNG, JPEG, PDF).\n",
    "##### 3. System generates and saves the report in the specified format.\n",
    "###### Postconditions: Comprehensive statistical report is available for review and sharing.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Non-functional requirements**\n",
    "##### **Usability**\n",
    "##### Usability refers to how well something is fit to be used. The program should be able to perform a number of tasks, such as Data Loading, Data Cleaning, Data Analysis, Data visualization, Data reporting without any errors. The user-interface should also be easy and simple to use, with minimal effort to switch between tasks to provide a smooth user experience, and to save time. The project should also contain a READ ME document, which will inform the user on information that they should know such as instructions on how to use the program, navitagate the UI, the purpose of the program and anything else of importance. \n",
    "##### **Reliability**\n",
    "##### Reliability the degree to which the result of a measurement, calculation, or specification can be depended on to be accurate. Both the program and the user-interface should be reliable in the sense that it should be mostly free from bugs and glitches and should perform as advertised. This will ensure a better user experience and reduce the uncertainty around its outputs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research of chosen issue \n",
    "##### The purpose of this project is to analyze the impact of COVID-19 on different countries and geographical regions. By examining how various regions faced the pandemic and the effects it had, the project aims to uncover patterns and insights that can inform public health responses, policy decisions, and future pandemic preparedness. Conducting this project is essential because the raw dataset, while comprehensive, does not provide meaningful insights on its own. The data is a collection of numbers and figures that require processing, analysis, and visualization to make sense of the underlying trends and patterns. Without a structured approach to clean, analyze, and visualize the data, it remains difficult to draw useful conclusions or actionable insights. \n",
    "\n",
    "#### Stakeholders\n",
    "###### Stakeholders for this project include:\n",
    "###### 1. Public Health Authorities: They can use the insights to improve public health strategies and responses in the event of future pandemics.\n",
    "###### 2. Researchers: Researches can use this data to further study the effects of this pandemic and suggest new research areas\n",
    "###### 3. General Public: The general public can use this research to better inform themselves on how different geographical regions were impacted by Covid-19 and help themselves better prepare in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Privacy and Security Considerations\n",
    "#### Data privacy of source:\n",
    "###### The dataset used in this project is sourced from Kaggle, which compiles publicly available information about COVID-19 cases across various countries. This data includes statistics such as case counts, recoveries, and deaths. Since the dataset is gathered at a global level, it does not contain personally identifiable information about individuals. As for the responsibilites for the data source, Kaggle and the original data providers must comply with data protection laws such as the General Data Protection Regulation (GDPR) in Europe, which mandates the protection of individuals' privacy.\n",
    "#### Application Data privacy:\n",
    "###### As a developer working with this dataset, if I decided to push this data out to the public, it is my responsiblity to maintain user privacy and handle the data responsbily. This includes being transparent with how I use this data as well as ensuring I do not use this data in malicious ways, furthermore, this includes making sure no personally identifiable info gets revealed to the public without consent.\n",
    "#### Cyber Security:\n",
    "##### Features an application such as this one should have to maintain cybersecurity once it's online include user identification. This involves verifying the identity of users before granting access to the application or specific features within it. User identification usually includes a username and password, which are used to authenticate one's identity, or user authentication. Moreover, Two-Factor Authentication (2FA) would be useful: Adding an extra layer of security by requiring a second form of verification, such as a code sent to a mobile device. Another crucial feature for an application to have is encryption, Encryption is the process of converting data into a coded format that can only be read by someone with the appropriate decryption key. It protects sensitive data from unauthorized access, whether the data is stored on a disk (data at rest) or being transmitted over a network (data in transit). It works by using keys to transform readable data (plaintext) into an unreadable format (ciphertext). Only those with the correct decryption key can convert the ciphertext back into plaintext, ensuring data privacy and security. Lastly, password hashing, the process of converting a plain text password into a fixed-length string of characters, which then gets stored in a database. Hashing algorithms are one-way functions, meaning they cannot be easily reversed to reveal the original password. It works by using algorithms such as bcrypt, then, when the user logs in, their inputted password is hashed again, and the system compares it with the stored hash. If they match, the user is authenticated. Hashing ensures that even if the database is compromised, the original passwords are not easily retrievable.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field | Datatype | Format for Display | Description | Example | Validation | \n",
    "| :- | -: | :-: | :- | -: | :-: |\n",
    "| Country/Reigon | Object | XXâ€¦XX | Name of the country impacted | India | Can be as many characters possible, but cannot include numbers\n",
    "| Confirmed |  Integer64 | I... | Amount of confirmed COVID cases | 5235 | Must be a whole number\n",
    "| Deaths | Integer64 | I... | Amount of confirmed deaths from COVID | 13131 | Must be a whole number\n",
    "| Recovered | Integer64 | I... | Amount of successful recoveries from COVID | 123124 | Must be a whole number\n",
    "| Deaths / 100 Cases | Float64 | N.NN... | Average amount of deaths per 100 cases | 31.43 | Must be a decimal number to 2 places or less\n",
    "| Recovered / 100 Cases | Float64 | N.NN... | Average amount of recoveries per 100 cases | 31.51 | Must be a decimal number to 2 places or less\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
